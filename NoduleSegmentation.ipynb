{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOB01wk605C3OKCsGMY64wt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkilaAgnesS/lung-nodule-detection/blob/main/NoduleSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the implementation with:\n",
        "\n",
        "The full CTDataset class for handling CT images and masks\n",
        "Training and evaluation functions\n",
        "Helper functions for calculating the Dice coefficient (a common metric for segmentation)\n",
        "Functions for model saving/loading\n",
        "Visualization utilities\n",
        "An example of how to use the code with sample data\n",
        "\n",
        "The implementation showcases how to use a Swin Transformer-based UNet for CT image segmentation with edge detection enhancement via the Canny algorithm and feature pyramid network (FPN) for feature aggregation."
      ],
      "metadata": {
        "id": "3N6upFJdi1iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Swin Transformer UNet for CT Image Segmentation\n",
        "# Google Colab Implementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive for saving models and results\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "TR5PdAVZTC2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN0FuphWgDoW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import copy\n",
        "from skimage.transform import resize\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.patheffects as path_effects\n",
        "from skimage.metrics import normalized_mutual_information\n",
        "try:\n",
        "    from skimage.metrics import structural_similarity\n",
        "except:\n",
        "    from skimage.measure import compare_ssim as structural_similarity\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dataset class for loading .npy files\n",
        "class LungCTDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths=None, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.load(self.image_paths[idx])\n",
        "\n",
        "        # Resize to 512x512 if needed\n",
        "        if image.shape[0] != 512 or image.shape[1] != 512:\n",
        "            image = resize(image, (512, 512), preserve_range=True)\n",
        "\n",
        "        # Convert to float tensors and normalize image to [0, 1]\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        if len(image.shape) == 2:  # Add channel dimension if needed\n",
        "            image = image.unsqueeze(0)\n",
        "\n",
        "        # Handle extreme values and NaNs\n",
        "        image = torch.nan_to_num(image)\n",
        "        if torch.max(image) > 0:\n",
        "            image = image / torch.max(image)  # Normalize to [0, 1]\n",
        "\n",
        "        # Handle mask if available\n",
        "        if self.mask_paths is not None:\n",
        "            mask = np.load(self.mask_paths[idx])\n",
        "\n",
        "            # Resize if needed\n",
        "            if mask.shape[0] != 512 or mask.shape[1] != 512:\n",
        "                mask = resize(mask, (512, 512), preserve_range=True)\n",
        "\n",
        "            mask = torch.tensor(mask, dtype=torch.float32)\n",
        "            if len(mask.shape) == 2:  # Add channel dimension\n",
        "                mask = mask.unsqueeze(0)\n",
        "\n",
        "            # Convert mask to binary (0 or 1)\n",
        "            mask = (mask > 0.5).float()\n",
        "        else:\n",
        "            # If no mask is provided, return a dummy mask\n",
        "            mask = torch.zeros_like(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Utility function to create patches from an image\n",
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: Input image tensor of shape [B, C, H, W]\n",
        "        patch_size: Size of patches to extract\n",
        "        flatten_channels: If True, channels are flattened with patch dim\n",
        "    Returns:\n",
        "        Tensor of shape [B, num_patches, C*patch_size*patch_size] if flatten_channels=True\n",
        "        else [B, num_patches, C, patch_size, patch_size]\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H', W', C, p, p]\n",
        "    x = x.flatten(1, 2)  # [B, H'*W', C, p, p]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2, 4)  # [B, H'*W', C*p*p]\n",
        "    return x\n",
        "\n",
        "# Patch Embedding layer\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, emb_dim, patch_size, img_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: [B, C, H, W]\"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.projection(x)  # [B, emb_dim, H/patch_size, W/patch_size]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, emb_dim]\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Window Attention Module\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # Linear projection layers\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        # Relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
        "        )\n",
        "\n",
        "        # Pair-wise relative position indices\n",
        "        coords_h = torch.arange(self.window_size)\n",
        "        coords_w = torch.arange(self.window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
        "        coords_flatten = torch.flatten(coords, 1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: [num_windows*B, window_size*window_size, dim]\"\"\"\n",
        "        B_, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each [B_, num_heads, N, head_dim]\n",
        "\n",
        "        # Compute attention\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # [B_, num_heads, N, N]\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size * self.window_size, self.window_size * self.window_size, -1\n",
        "        )\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [num_heads, N, N]\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        # Weighted aggregation\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Window partitioning function\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [B, H, W, C]\n",
        "        window_size: window size\n",
        "    Returns:\n",
        "        windows: [num_windows*B, window_size, window_size, C]\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "\n",
        "    # Ensure dimensions are divisible by window_size\n",
        "    pad_h = (window_size - H % window_size) % window_size\n",
        "    pad_w = (window_size - W % window_size) % window_size\n",
        "\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
        "        H, W = H + pad_h, W + pad_w\n",
        "\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "# Window reverse function\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: [num_windows*B, window_size, window_size, C]\n",
        "        window_size: Window size\n",
        "        H, W: Height and width of the image\n",
        "    Returns:\n",
        "        x: [B, H, W, C]\n",
        "    \"\"\"\n",
        "    # Ensure H and W are divisible by window_size\n",
        "    H_orig, W_orig = H, W\n",
        "    pad_h = (window_size - H % window_size) % window_size\n",
        "    pad_w = (window_size - W % window_size) % window_size\n",
        "    H, W = H + pad_h, W + pad_w\n",
        "\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "\n",
        "    # Remove padding if added\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "        x = x[:, :H_orig, :W_orig, :]\n",
        "\n",
        "    return x\n",
        "\n",
        "# MLP Block\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "# Swin Transformer Block\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Window Attention\n",
        "        self.attn = WindowAttention(dim, window_size, num_heads)\n",
        "\n",
        "        # MLP\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "        # Handling boundaries when shifting windows\n",
        "        self.H = None\n",
        "        self.W = None\n",
        "\n",
        "    def forward(self, x, mask_matrix=None):\n",
        "        \"\"\"x: [B, L, C]\"\"\"\n",
        "        B, L, C = x.shape\n",
        "\n",
        "        # Determine H and W ensuring they're square to avoid dimension issues\n",
        "        H = W = int(math.sqrt(L))\n",
        "\n",
        "        # If L is not a perfect square, we need to pad\n",
        "        if H * W != L:\n",
        "            nearest_sq = int(math.ceil(math.sqrt(L)))\n",
        "            H = W = nearest_sq\n",
        "            # Pad the sequence to make it a perfect square\n",
        "            padding = H * W - L\n",
        "            if padding > 0:\n",
        "                padding_tensor = torch.zeros(B, padding, C, device=x.device, dtype=x.dtype)\n",
        "                x = torch.cat([x, padding_tensor], dim=1)\n",
        "                L = H * W\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cache size for later use\n",
        "        self.H, self.W = H, W\n",
        "\n",
        "        # Skip connection\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)  # [B, H, W, C]\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # [num_windows*B, window_size, window_size, C]\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [num_windows*B, window_size*window_size, C]\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows)  # [num_windows*B, window_size*window_size, C]\n",
        "\n",
        "        # Merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # [B, H, W, C]\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Reshape back to [B, L, C]\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # If we padded earlier, remove the padding\n",
        "        if H * W > L:\n",
        "            x = x[:, :L, :]\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut.view(B, -1, C) + x\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Patch Merging layer\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.reduction = nn.Linear(4 * in_dim, out_dim, bias=False)\n",
        "        self.norm = nn.LayerNorm(4 * in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: [B, H*W, C]\"\"\"\n",
        "        B, L, C = x.shape\n",
        "        H = W = int(math.sqrt(L))\n",
        "\n",
        "        # Reshape to [B, H, W, C]\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Concatenate 2x2 patches\n",
        "        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n",
        "        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n",
        "        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n",
        "        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n",
        "        x = torch.cat([x0, x1, x2, x3], dim=-1)  # [B, H/2, W/2, 4*C]\n",
        "        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]\n",
        "\n",
        "        # Linear projection\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Patch Expanding layer\n",
        "class PatchExpanding(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.expansion = nn.Linear(in_dim, scale_factor * scale_factor * out_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: [B, H*W, C]\"\"\"\n",
        "        B, L, C = x.shape\n",
        "        H = W = int(math.sqrt(L))\n",
        "\n",
        "        # Expand features\n",
        "        x = self.expansion(x)  # [B, H*W, scale_factor*scale_factor*out_dim]\n",
        "\n",
        "        # Reshape and rearrange to increase spatial size\n",
        "        x = x.view(B, H, W, self.scale_factor, self.scale_factor, -1)\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "        out_dim = x.shape[-1]\n",
        "        x = x.view(B, H * self.scale_factor, W * self.scale_factor, out_dim)\n",
        "        x = x.view(B, -1, out_dim)  # [B, (H*scale_factor)*(W*scale_factor), out_dim]\n",
        "\n",
        "        return x\n",
        "\n",
        "# Basic Swin Transformer Layer (multiple blocks with skip connection)\n",
        "class BasicLayer(nn.Module):\n",
        "    def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "\n",
        "        # Build Swin Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                window_size=window_size,\n",
        "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                mlp_ratio=mlp_ratio\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "# Encoder part of Swin Transformer U-Net\n",
        "class SwinTransformerEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, depths=[2, 2, 2], embed_dims=[96, 192, 384], num_heads=[3, 6, 12], window_size=7):\n",
        "        super().__init__()\n",
        "        self.depths = depths\n",
        "        self.num_stages = len(depths)\n",
        "\n",
        "        # Initial patch embedding - use patch_size=8 for 512x512 images\n",
        "        self.patch_embed = PatchEmbedding(in_channels, embed_dims[0], patch_size=8, img_size=512)\n",
        "\n",
        "        # Swin Transformer layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(self.num_stages):\n",
        "            layer = BasicLayer(\n",
        "                dim=embed_dims[i],\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                window_size=window_size\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "            # Add patch merging except for the last layer\n",
        "            if i < self.num_stages - 1:\n",
        "                self.layers.append(PatchMerging(embed_dims[i], embed_dims[i+1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)  # [B, L, C]\n",
        "\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, BasicLayer):\n",
        "                features.append(x)\n",
        "\n",
        "        return features\n",
        "\n",
        "# Decoder part of Swin Transformer U-Net\n",
        "class SwinTransformerDecoder(nn.Module):\n",
        "    def __init__(self, depths=[2, 2, 2], embed_dims=[384, 192, 96], num_heads=[12, 6, 3], window_size=7):\n",
        "        super().__init__()\n",
        "        self.depths = depths\n",
        "        self.num_stages = len(depths)\n",
        "\n",
        "        # Layers for decoding (in reverse order)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(self.num_stages):\n",
        "            # Patch Expanding\n",
        "            self.layers.append(PatchExpanding(embed_dims[i], embed_dims[min(i+1, len(embed_dims)-1)]))\n",
        "\n",
        "            # Swin Transformer Block\n",
        "            self.layers.append(BasicLayer(\n",
        "                dim=embed_dims[min(i+1, len(embed_dims)-1)],\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                window_size=window_size\n",
        "            ))\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Start from the deepest feature map\n",
        "        x = features[-1]\n",
        "\n",
        "        # Process through layers with skip connections\n",
        "        for i in range(0, len(self.layers), 2):\n",
        "            # Expanding\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "            # Skip connection (if available)\n",
        "            if i//2 < len(features) - 1:\n",
        "                x = x + features[len(features) - 2 - i//2]\n",
        "\n",
        "            # Swin transformer blocks\n",
        "            x = self.layers[i+1](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Feature Pyramid Network (FPN)\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Lateral connections\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Linear(in_channels, out_channels)\n",
        "            for in_channels in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # Output convolutions\n",
        "        self.output_convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(out_channels, out_channels),\n",
        "                nn.LayerNorm(out_channels),\n",
        "                nn.GELU()\n",
        "            )\n",
        "            for _ in in_channels_list\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"features: List of feature maps from different levels\"\"\"\n",
        "\n",
        "        # Process each feature map\n",
        "        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n",
        "\n",
        "        # Top-down pathway\n",
        "        for i in range(len(laterals)-1, 0, -1):\n",
        "            # Ensure spatial dimensions match\n",
        "            B, L_high, C = laterals[i].shape\n",
        "            B, L_low, C = laterals[i-1].shape\n",
        "\n",
        "            H_high = W_high = int(math.sqrt(L_high))\n",
        "            H_low = W_low = int(math.sqrt(L_low))\n",
        "\n",
        "            # Reshape for interpolation\n",
        "            laterals[i] = laterals[i].view(B, H_high, W_high, C)\n",
        "            laterals[i] = F.interpolate(\n",
        "                laterals[i].permute(0, 3, 1, 2),\n",
        "                size=(H_low, W_low),\n",
        "                mode='bilinear',\n",
        "                align_corners=False\n",
        "            ).permute(0, 2, 3, 1).contiguous().view(B, L_low, C)\n",
        "\n",
        "            # Feature fusion\n",
        "            laterals[i-1] = laterals[i-1] + laterals[i]\n",
        "\n",
        "        # Apply output convolutions\n",
        "        outs = [output_conv(lateral) for lateral, output_conv in zip(laterals, self.output_convs)]\n",
        "\n",
        "        return outs\n",
        "\n",
        "# Complete Swin Transformer Segmentation Model\n",
        "class SwinTransformerSegmentation(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, depths=[2, 2, 2],\n",
        "                 embed_dims=[96, 192, 384], num_heads=[3, 6, 12], window_size=7):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = SwinTransformerEncoder(\n",
        "            in_channels=in_channels,\n",
        "            depths=depths,\n",
        "            embed_dims=embed_dims,\n",
        "            num_heads=num_heads,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = SwinTransformerDecoder(\n",
        "            depths=depths[::-1],\n",
        "            embed_dims=embed_dims[::-1],\n",
        "            num_heads=num_heads[::-1],\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        # Feature Pyramid Network\n",
        "        self.fpn = FPN(embed_dims, embed_dims[0])\n",
        "\n",
        "        # Edge detection branch\n",
        "        self.edge_detection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Final prediction layers\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Linear(embed_dims[0], 64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Edge detection branch\n",
        "        edge_features = self.edge_detection(x)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_features = self.encoder(x)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_output = self.decoder(encoder_features)\n",
        "\n",
        "        # FPN for feature aggregation\n",
        "        fpn_features = self.fpn([encoder_features[0], encoder_features[1], encoder_features[2]])\n",
        "\n",
        "        # Final prediction\n",
        "        B, L, C = decoder_output.shape\n",
        "        H = W = int(math.sqrt(L))\n",
        "\n",
        "        # Reshape to spatial dimensions\n",
        "        out = self.final_conv(decoder_output)  # [B, L, out_channels]\n",
        "        out = out.view(B, H, W, -1).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Upsample to input resolution\n",
        "        out = F.interpolate(out, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Combine with edge features\n",
        "        out = out + edge_features\n",
        "\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, masks in train_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        dice_score = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate Dice coefficient\n",
        "                pred = (outputs > 0.5).float()\n",
        "                intersection = (pred * masks).sum().item()\n",
        "                dice = (2 * intersection) / (pred.sum().item() + masks.sum().item() + 1e-8)\n",
        "                dice_score += dice\n",
        "\n",
        "        # Calculate average validation loss and Dice score\n",
        "        val_loss /= len(val_loader)\n",
        "        dice_score /= len(val_loader)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Dice: {dice_score:.4f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f'Model saved at epoch {epoch+1}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Inference function with DSC score overlay\n",
        "def predict(model, image_path, mask_path=None, output_path=None):\n",
        "    \"\"\"\n",
        "    Run inference and create visualization with DSC score overlay if mask is available\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.patches import Rectangle\n",
        "    import matplotlib.patheffects as path_effects\n",
        "\n",
        "    # Load image\n",
        "    image = np.load(image_path)\n",
        "\n",
        "    # Preprocess\n",
        "    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
        "    image_tensor = image_tensor / torch.max(image_tensor)  # Normalize\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        output = model(image_tensor)\n",
        "        prediction = (output > 0.5).float().cpu().numpy().squeeze()\n",
        "\n",
        "    # Calculate evaluation metrics if mask is available\n",
        "    dsc_score = None\n",
        "    if mask_path:\n",
        "        try:\n",
        "            # Load mask\n",
        "            true_mask = np.load(mask_path)\n",
        "\n",
        "            # Ensure it's binary\n",
        "            true_mask = (true_mask > 0.5).astype(np.float32)\n",
        "\n",
        "            # Calculate Dice coefficient\n",
        "            smooth = 1e-6\n",
        "            intersection = np.sum(prediction * true_mask)\n",
        "            dsc_score = (2. * intersection + smooth) / (np.sum(prediction) + np.sum(true_mask) + smooth)\n",
        "\n",
        "            # Calculate other metrics\n",
        "            nmi_score = normalized_mutual_information(true_mask, prediction)\n",
        "            try:\n",
        "                ssim_score = structural_similarity(true_mask, prediction)\n",
        "            except:\n",
        "                ssim_score = 0.0\n",
        "\n",
        "            print(f\"Metrics: DSC={dsc_score:.4f}, NMI={nmi_score:.4f}, SSIM={ssim_score:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics: {str(e)}\")\n",
        "\n",
        "    # Create visualization similar to the example\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Show the original CT image with prediction overlay\n",
        "    plt.imshow(image, cmap='gray')\n",
        "\n",
        "    # Add segmentation overlay with transparency\n",
        "    mask_overlay = np.zeros((*prediction.shape, 4))\n",
        "    mask_overlay[prediction > 0.5] = [1, 0, 0, 0.3]  # Red with transparency\n",
        "    plt.imshow(mask_overlay)\n",
        "\n",
        "    # Add DSC score in a box if available\n",
        "    if dsc_score is not None:\n",
        "        # Add box in top-left corner\n",
        "        ax = plt.gca()\n",
        "        box = Rectangle((5, 5), 100, 30, facecolor='black', alpha=0.7)\n",
        "        ax.add_patch(box)\n",
        "\n",
        "        # Add DSC score text\n",
        "        text = plt.text(10, 25, f\"DSC: {dsc_score:.2f}\", color='white', fontsize=12, weight='bold')\n",
        "\n",
        "        # Add outline to make text more readable\n",
        "        text.set_path_effects([\n",
        "            path_effects.Stroke(linewidth=2, foreground='black'),\n",
        "            path_effects.Normal()\n",
        "        ])\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if output path is provided\n",
        "    if output_path:\n",
        "        plt.savefig(output_path, dpi=200, bbox_inches='tight')\n",
        "        print(f\"Saved visualization to {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced lung nodule detection model with focused training\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.patheffects as path_effects\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "###### STEP 1: LOAD AND PREPARE THE DATA ######\n",
        "\n",
        "# Path to your .npy files\n",
        "image_path = '/content/drive/My Drive/Lung/2D_img_512.npy'\n",
        "mask_path = '/content/drive/My Drive/Lung/2D_mask_512.npy'\n",
        "\n",
        "# Load the data\n",
        "images = np.load(image_path)\n",
        "masks = np.load(mask_path)\n",
        "print(f\"Loaded {images.shape[0]} images with shape {images.shape[1:]}\")\n",
        "print(f\"Loaded {masks.shape[0]} masks with shape {masks.shape[1:]}\")\n",
        "\n",
        "# Find images with significant nodules (where mask has substantial positive area)\n",
        "nodule_indices = []\n",
        "for i in range(len(masks)):\n",
        "    mask_area = np.sum(masks[i] > 0.5)\n",
        "    if mask_area > 100:  # Threshold to ensure the nodule is substantial\n",
        "        nodule_indices.append(i)\n",
        "\n",
        "print(f\"Found {len(nodule_indices)} images with significant nodules\")\n",
        "\n",
        "# If we have very few images with nodules, adjust the threshold\n",
        "if len(nodule_indices) < 10:\n",
        "    nodule_indices = []\n",
        "    for i in range(len(masks)):\n",
        "        mask_area = np.sum(masks[i] > 0.5)\n",
        "        if mask_area > 0:  # Any nodule\n",
        "            nodule_indices.append(i)\n",
        "    print(f\"Adjusted threshold: Found {len(nodule_indices)} images with any nodules\")\n",
        "\n",
        "# Select a sample with a clear nodule for later inference\n",
        "if nodule_indices:\n",
        "    # Find the image with the largest nodule\n",
        "    max_nodule_idx = max(nodule_indices, key=lambda i: np.sum(masks[i] > 0.5))\n",
        "    sample_idx = max_nodule_idx\n",
        "else:\n",
        "    sample_idx = 100  # Default\n",
        "\n",
        "sample_image = images[sample_idx].copy()\n",
        "sample_mask = masks[sample_idx].copy()\n",
        "\n",
        "print(f\"Selected sample index {sample_idx} with nodule area: {np.sum(sample_mask > 0.5)} pixels\")\n",
        "\n",
        "# Visualize the sample image and mask\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sample_image, cmap='gray')\n",
        "plt.title(f'Sample Image (Index: {sample_idx})')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(sample_mask, cmap='gray')\n",
        "plt.title('Nodule Mask')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prepare dataset for training with class weighting for better nodule detection\n",
        "class LungNoduleDataset(Dataset):\n",
        "    def __init__(self, images, masks, transform=None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        # Normalize image to [0,1]\n",
        "        if np.max(image) > 0:\n",
        "            image = image / np.max(image)\n",
        "\n",
        "        # Ensure mask is binary\n",
        "        mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
        "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)    # Add channel dimension\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Use nodule indices to ensure we train primarily on images with nodules\n",
        "if nodule_indices:\n",
        "    # Split nodule indices into training and validation\n",
        "    train_nodule_indices, val_nodule_indices = train_test_split(\n",
        "        nodule_indices, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Add some non-nodule images for balanced training\n",
        "    non_nodule_indices = [i for i in range(len(images)) if i not in nodule_indices]\n",
        "    if non_nodule_indices:\n",
        "        # Add equal number of non-nodule images\n",
        "        num_non_nodules = min(len(train_nodule_indices), len(non_nodule_indices))\n",
        "        selected_non_nodules = np.random.choice(non_nodule_indices, num_non_nodules, replace=False)\n",
        "        train_indices = list(train_nodule_indices) + list(selected_non_nodules)\n",
        "\n",
        "        # Add some non-nodule images to validation\n",
        "        num_val_non_nodules = min(len(val_nodule_indices), len(non_nodule_indices))\n",
        "        val_non_nodule_indices = np.random.choice(\n",
        "            [i for i in non_nodule_indices if i not in selected_non_nodules],\n",
        "            num_val_non_nodules, replace=False\n",
        "        )\n",
        "        val_indices = list(val_nodule_indices) + list(val_non_nodule_indices)\n",
        "    else:\n",
        "        train_indices = train_nodule_indices\n",
        "        val_indices = val_nodule_indices\n",
        "else:\n",
        "    # If no clear nodules, use regular train/val split\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(images)), test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"Training on {len(train_indices)} images ({len([i for i in train_indices if i in nodule_indices])} with nodules)\")\n",
        "print(f\"Validating on {len(val_indices)} images ({len([i for i in val_indices if i in nodule_indices])} with nodules)\")\n",
        "\n",
        "# Create the actual training and validation datasets\n",
        "train_images = np.array([images[i] for i in train_indices])\n",
        "train_masks = np.array([masks[i] for i in train_indices])\n",
        "val_images = np.array([images[i] for i in val_indices])\n",
        "val_masks = np.array([masks[i] for i in val_indices])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LungNoduleDataset(train_images, train_masks)\n",
        "val_dataset = LungNoduleDataset(val_images, val_masks)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "###### STEP 2: DEFINE A BETTER MODEL FOR NODULE DETECTION ######\n",
        "\n",
        "# U-Net model for better nodule segmentation\n",
        "# U-Net model for better nodule segmentation - Fixed version\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self._make_encoder_block(in_channels, 64)\n",
        "        self.enc2 = self._make_encoder_block(64, 128)\n",
        "        self.enc3 = self._make_encoder_block(128, 256)\n",
        "        self.enc4 = self._make_encoder_block(256, 512)\n",
        "\n",
        "        # Bridge\n",
        "        self.bridge = self._make_encoder_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = self._make_decoder_block(1024, 512)  # 1024 = 512 + 512 (skip)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = self._make_decoder_block(512, 256)   # 512 = 256 + 256 (skip)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self._make_decoder_block(256, 128)   # 256 = 128 + 128 (skip)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = self._make_decoder_block(128, 64)    # 128 = 64 + 64 (skip)\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "        # Max pooling for encoder\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def _make_encoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "\n",
        "        x = self.pool(enc1)\n",
        "        enc2 = self.enc2(x)\n",
        "\n",
        "        x = self.pool(enc2)\n",
        "        enc3 = self.enc3(x)\n",
        "\n",
        "        x = self.pool(enc3)\n",
        "        enc4 = self.enc4(x)\n",
        "\n",
        "        # Bridge\n",
        "        x = self.pool(enc4)\n",
        "        bridge = self.bridge(x)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.upconv4(bridge)\n",
        "        x = torch.cat([x, enc4], dim=1)\n",
        "        x = self.dec4(x)\n",
        "\n",
        "        x = self.upconv3(x)\n",
        "        x = torch.cat([x, enc3], dim=1)\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        x = self.upconv2(x)\n",
        "        x = torch.cat([x, enc2], dim=1)\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        x = self.upconv1(x)\n",
        "        x = torch.cat([x, enc1], dim=1)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        # Final output\n",
        "        return torch.sigmoid(self.final(x))\n",
        "\n",
        "# Initialize and train the model\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "model = train_model(model, train_loader, val_loader, num_epochs=15)  # More epochs for better training\n",
        "\n",
        "###### STEP 4: RUN INFERENCE WITH IMPROVED VISUALIZATION ######\n",
        "\n",
        "def predict_with_dsc_overlay(model, image, mask=None):\n",
        "    \"\"\"Run inference and create visualization with DSC score overlay\"\"\"\n",
        "    # Prepare image for the model\n",
        "    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
        "    if np.max(image) > 0:\n",
        "        image_tensor = image_tensor / torch.max(image_tensor)  # Normalize\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        output = model(image_tensor)\n",
        "        prediction_prob = output.cpu().numpy().squeeze()  # Keep probabilities\n",
        "        prediction = (prediction_prob > 0.5).astype(np.float32)  # Binary prediction\n",
        "\n",
        "    # Calculate evaluation metrics if mask is provided\n",
        "    dsc_score = None\n",
        "    if mask is not None:\n",
        "        # Ensure mask is binary\n",
        "        true_mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "        # Calculate Dice coefficient\n",
        "        smooth = 1e-6\n",
        "        intersection = np.sum(prediction * true_mask)\n",
        "        dsc_score = (2. * intersection + smooth) / (np.sum(prediction) + np.sum(true_mask) + smooth)\n",
        "        print(f\"DSC Score: {dsc_score:.4f}\")\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Show the original CT image\n",
        "    plt.imshow(image, cmap='gray')\n",
        "\n",
        "    # Add segmentation overlay with transparency\n",
        "    mask_overlay = np.zeros((*prediction.shape, 4))\n",
        "    mask_overlay[prediction > 0.5] = [1, 0, 0, 0.3]  # Red with transparency\n",
        "    plt.imshow(mask_overlay)\n",
        "\n",
        "    # Add DSC score in a box if available\n",
        "    if dsc_score is not None:\n",
        "        # Add box in top-left corner\n",
        "        ax = plt.gca()\n",
        "        box = Rectangle((5, 5), 100, 30, facecolor='black', alpha=0.7)\n",
        "        ax.add_patch(box)\n",
        "\n",
        "        # Add DSC score text\n",
        "        text = plt.text(10, 25, f\"DSC: {dsc_score:.2f}\", color='white', fontsize=12, weight='bold')\n",
        "\n",
        "        # Add outline to make text more readable\n",
        "        text.set_path_effects([\n",
        "            path_effects.Stroke(linewidth=2, foreground='black'),\n",
        "            path_effects.Normal()\n",
        "        ])\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save result\n",
        "    plt.savefig('/content/sample_prediction.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Create a detailed comparison figure with heatmap\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth mask\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title('Ground Truth')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Model prediction (binary)\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(prediction, cmap='gray')\n",
        "    plt.title(f'Prediction (DSC: {dsc_score:.2f})')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Probability heatmap\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(prediction_prob, cmap='hot')\n",
        "    plt.colorbar(label='Probability')\n",
        "    plt.title('Probability Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/detailed_comparison.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Run inference on the sample image\n",
        "print(\"\\n--- Running inference on sample image with nodule ---\")\n",
        "prediction = predict_with_dsc_overlay(model, sample_image, sample_mask)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/lung_nodule_detection_model.pth')\n",
        "print(\"Model saved to /content/lung_nodule_detection_model.pth\")\n",
        "print(\"Sample prediction saved to /content/sample_prediction.png\")\n",
        "print(\"Detailed comparison saved to /content/detailed_comparison.png\")"
      ],
      "metadata": {
        "id": "XuT2fYn9mv-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}